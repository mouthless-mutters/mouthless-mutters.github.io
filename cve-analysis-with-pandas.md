## Introduction 

I want to explore Pandas while doing some basic analysis on CVE data.

CVE data obtained from the CVE-2024 zip file at https://nvd.nist.gov/vuln/data-feeds#JSON_FEED

## Loading and inspecting the data

I start by importing pandas and reading in the zip file. 

```
import pandas as pd

cve_zip_file = "nvdcve-1.1-2024.json.zip"

df = pd.read_json(cve_zip_file)
```

To interact with the script in the repl, I run the code using the -i flag. 
```
> python -i .\cve_analyzer.py
```

To determine the number of rows and columns, you can use df.shape
```
>>> df.shape
(34757, 6)
```

To get the names of the columns, use df.columns
```
>>> df.columns
Index(['CVE_data_type', 'CVE_data_format', 'CVE_data_version',  
       'CVE_data_numberOfCVEs', 'CVE_data_timestamp', 'CVE_Items']
```

The interesting data is in the CVE_Items column, which is a nested dictionary. Ultimately, we would like all of the data un-nested and read in as a dataframe. 

To do this, use pd.normalize_json on the CVE_Items column.
```
>>> df = pd.json_normalize(df['CVE_Items'])
>>> df.shape
(34757, 26)
>>> df.columns
Index(['publishedDate', 'lastModifiedDate', 'cve.data_type', 'cve.data_format',
       'cve.data_version', 'cve.CVE_data_meta.ID',
       'cve.CVE_data_meta.ASSIGNER', 'cve.problemtype.problemtype_data',
       'cve.references.reference_data', 'cve.description.description_data',
       'configurations.CVE_data_version', 'configurations.nodes',
       'impact.baseMetricV3.cvssV3.version',
       'impact.baseMetricV3.cvssV3.vectorString',
       'impact.baseMetricV3.cvssV3.attackVector',
       'impact.baseMetricV3.cvssV3.attackComplexity',
       'impact.baseMetricV3.cvssV3.privilegesRequired',
       'impact.baseMetricV3.cvssV3.userInteraction',
       'impact.baseMetricV3.cvssV3.scope',
       'impact.baseMetricV3.cvssV3.confidentialityImpact',
       'impact.baseMetricV3.cvssV3.integrityImpact',
       'impact.baseMetricV3.cvssV3.availabilityImpact',
       'impact.baseMetricV3.cvssV3.baseScore',
       'impact.baseMetricV3.cvssV3.baseSeverity',
       'impact.baseMetricV3.exploitabilityScore',
       'impact.baseMetricV3.impactScore'],
      dtype='object')
>>>
```
Unfortuneately, some additional nested json still exists, but this is a good enough start for some initial exploration. To help see all of the different data in the various columns, I prefer to export the dataframe to a CSV.
```
>>> df.to_csv('2024_CVE_data').csv
```
However, there are multiple ways that pandas can be used to explore the data.

Below is just from chatgpt to keep me on track. I will update later

Here are some common terms and operations used when familiarizing yourself with a dataset in **Pandas**:

### 1. **Loading and Inspecting the Data**
   - **`pd.read_csv()`**, **`pd.read_excel()`**, **`pd.read_sql()`**, etc.: Load data from various sources.
   - **`.head()`**: View the first few rows of the dataset.
   - **`.tail()`**: View the last few rows of the dataset.
   - **`.info()`**: Display a concise summary of the dataset, including column names, non-null counts, and data types.
   - **`.describe()`**: Generate summary statistics for numerical columns.
   - **`.columns`**: Get the list of column names.
   - **`.shape`**: Check the dimensions of the dataset (rows and columns).
   - **`.dtypes`**: View the data types of columns.
   - **`.isnull()` / `.notnull()`**: Check for missing values.

### 2. **Selecting and Filtering Columns**
   - **Indexing (`df['column_name']` or `df[['col1', 'col2']]`)**: Select specific columns.
   - **`.iloc[]` / `.loc[]`**: Select rows and columns using indices or labels.
   - **`.filter()`**: Select columns or rows that match a pattern.

### 3. **Handling Missing Values**
   - **`.isna()` / `.isnull()`**: Identify missing values.
   - **`.fillna()`**: Fill missing values with a specified value (e.g., mean, median, mode).
   - **`.dropna()`**: Remove rows or columns containing missing values.
   - **`.interpolate()`**: Fill missing values using interpolation.

### 4. **Removing or Modifying Columns**
   - **`.drop(columns=['col1', 'col2'])`**: Remove unnecessary columns.
   - **`.rename()`**: Rename columns for clarity.

### 5. **Transforming Data**
   - **`.apply()`**: Apply a function to columns or rows.
   - **`.map()`**: Apply a function element-wise to a column.
   - **`.assign()`**: Create or assign new columns.
   - **`.str`**: Perform string-based operations (e.g., `.str.lower()`, `.str.split()`, `.str.contains()`).

### 6. **Feature Engineering**
   - **Extracting Data**: Use string methods (`.str.split()`, `.str.extract()`) or mathematical operations to derive new columns from existing ones.
   - **Creating New Columns**: Use mathematical or logical operations to create new features (e.g., `df['new_col'] = df['col1'] + df['col2']`).

### 7. **Sorting and Reorganizing**
   - **`.sort_values()`**: Sort by one or more columns.
   - **`.sort_index()`**: Sort by the index.
   - **`.reset_index()`**: Reset the index after filtering or reordering.

### 8. **Summarizing Data**
   - **`.value_counts()`**: Count unique values in a column.
   - **`.unique()`**: List unique values in a column.
   - **`.nunique()`**: Count the number of unique values.
   - **`.groupby()`**: Aggregate data based on one or more columns.
   - **`.pivot_table()` / `.crosstab()`**: Create pivot tables for summarization.

### 9. **Filtering and Subsetting**
   - **Conditional Filtering**: `df[df['col'] > value]`, `df[df['col'].str.contains('pattern')]`
   - **`.query()`**: Filter rows using a query string (e.g., `df.query('col1 > 5 and col2 == "value"')`).

### 10. **Data Type Conversion**
   - **`.astype()`**: Convert a column to a specific data type.
   - **`.to_datetime()`**: Convert a column to datetime format.
   - **`.to_numeric()`**: Convert a column to numeric format, coercing invalid values.

### 11. **Indexing**
   - **`.set_index()`**: Set a specific column as the index.
   - **`.reset_index()`**: Reset the index to default integer-based indexing.

### 12. **Exporting Data**
   - **`.to_csv()`**, **`.to_excel()`**, etc.: Save the dataset to a file.

By mastering these operations and terms, you can efficiently load, clean, explore, and transform datasets in Pandas.

